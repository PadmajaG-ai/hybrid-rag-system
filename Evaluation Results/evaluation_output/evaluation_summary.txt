======================================================================
RAG SYSTEM EVALUATION SUMMARY
======================================================================

Evaluation Date: 2026-02-07 23:14:21
Input File: qa_pairs.json
Total Questions: 100
Output Directory: evaluation_output

EXECUTION TIME
----------------------------------------------------------------------
Evaluation          : 9.8 minutes
Llm Judge           : 0.2 minutes
Reports             : 0.0 minutes
Total               : 10.0 minutes

METHOD COMPARISON
----------------------------------------------------------------------

Performance by Method:

DENSE:
   MRR:              0.4237
   F1 Score:         0.0903
   ROUGE-L:          0.0981
   Semantic Sim:     0.2246
   Grounding Score:  0.4441

SPARSE:
   MRR:              0.2781
   F1 Score:         0.0903
   ROUGE-L:          0.0981
   Semantic Sim:     0.2246
   Grounding Score:  0.3537

HYBRID:
   MRR:              0.4165
   F1 Score:         0.0903
   ROUGE-L:          0.0981
   Semantic Sim:     0.2246
   Grounding Score:  0.4559

Hybrid Improvements:
   vs Dense:  MRR -1.70%, F1 +0.00%
   vs Sparse: MRR +49.76%, F1 +0.00%

LLM-AS-JUDGE SCORES
----------------------------------------------------------------------
   Factual Accuracy: 1.97/5.0
   Completeness:     4.68/5.0
   Relevance:        5.00/5.0
   Coherence:        3.99/5.0
   Overall Score:    3.79/5.0

======================================================================
OUTPUT FILES
======================================================================

✓ Evaluation Results (JSON)      evaluation_results.json        (2063.5 KB)
✓ LLM Judge Results (JSON)       judge_results.json             (2132.4 KB)
✗ HTML Report                    (not created)
✗ CSV Report                     (not created)
✓ Summary (TXT)                  evaluation_summary.txt         (0.0 KB)

======================================================================
NEXT STEPS
======================================================================

1. View summary:
   cat evaluation_output\evaluation_summary.txt

2. Open HTML report:
   open evaluation_output\evaluation_report.html

3. Launch interactive dashboard:
   streamlit run evaluation_dashboard_with_methods.py
   (Select: evaluation_output/judge_results.json)

4. Test live RAG demo:
   streamlit run rag_ui_flan.py

